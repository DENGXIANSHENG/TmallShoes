# -*- coding: utf-8 -*-
import re

import scrapy
from tmallshoes import settings
from selenium import webdriver
from selenium.webdriver.support.wait import WebDriverWait

from tmallshoes.items import TmallshoesItem


class TmallshoesSpider(scrapy.Spider):
    name = 'tmallshoes'

    def __init__(self):
        super(TmallshoesSpider, self).__init__()
        self.start_urls = ['https://www.tmall.com']
        self.allowed_domain = ['www.tmall.com']
        '''
        option = webdriver.ChromeOptions()
        option.add_argument('headless')
        option.add_argument('--disable-gpu')
        option.add_argument(settings.USER_AGENT)
        self.browser = webdriver.Chrome(executable_path=r'C:/ProgramData/Anaconda3/Library/bin/chromedriver.exe',
                                        chrome_options=option)
        # self.browser.set_page_load_timeout(10)
        '''
        self.browser = webdriver.Chrome(r'C:/ProgramData/Anaconda3/Library/bin/chromedriver.exe')
        self.wait = WebDriverWait(self.browser, 1000)
        self.start_page = 1

    def parse(self, response):
        url_set = set()
        self.browser.get(response.url)
        input = self.wait.until(
            lambda browser: browser.find_element_by_xpath('//*[@id="mq"]')
        )
        input.send_keys('女鞋')
        submit = self.wait.until(
            lambda browser: browser.find_element_by_xpath('//*[@id="mallSearch"]/form/fieldset/div/button')
        )
        submit.click()
        input_price = self.wait.until(
            lambda browser: browser.find_element_by_xpath('//*[@id="J_FPrice"]/div[1]/b[1]/input')
        )
        input_price.clear()
        input_price.send_keys('2000')
        submit_price = self.wait.until(
            lambda browser: browser.find_element_by_xpath('//*[@id="J_FPEnter"]')
        )
        submit_price.click()

        sort = self.wait.until(
            lambda browser: browser.find_element_by_xpath('//*[@id="J_Filter"]/a[4]')
        )
        sort.click()
        url = self.browser.current_url
        url_set.add(url)
        for i in range(2, 10):
            next_input = self.wait.until(
                lambda browser: browser.find_element_by_xpath(
                    '//*[@id="content"]/div/div[@class="ui-page"]/div/b[@class="ui-page-skip"]/form/input[@name="jumpto"]')
            )
            next_input.clear()
            next_input.send_keys(i)
            next_submit = self.wait.until(
                lambda browser: browser.find_element_by_xpath(
                    '//*[@id="content"]/div/div[@class="ui-page"]/div/b[@class="ui-page-skip"]/form/button')
            )
            next_submit.click()
            url_set.add(self.browser.current_url)
        for url in url_set:
            print('-------------------------------------------------------------' + url)
            yield scrapy.Request(url, callback=self.parse_content)

    def parse_content(self, response):
        print('++++++++++++++++' + str(response))
        sites = response.xpath('//*[@id="J_ItemList"]/div[@class="product  "]/div[@class="product-iWrap"]')
        products = []
        for site in sites:
            product = TmallshoesItem()
            title = site.xpath(
                './p[@class="productTitle"]/a/@title').extract()
            # product['title'] = [t for t in title]
            for t in title:
                product['title'] = t
            link = site.xpath(
                './div[@class="productImg-wrap"]/a/@href').extract()
            # product['link'] = [l for l in link]
            for l in link:
                product['link'] = 'https:' + l
            price = site.xpath(
                './p[@class="productPrice"]/em/text()').extract()
            # product['price'] = [p for p in price]
            for p in price:
                product['price'] = p
            deal = site.xpath(
                './p[@class="productStatus"]/span/em/text()').extract()
            # product['deal'] = [d for d in deal]
            for d in deal:
                product['deal'] = d[:-1]
            shop = site.xpath(
                './div[@class="productShop"]/a/text()').extract()
            # product['shop'] = [p for p in shop]
            for s in shop:
                product['shop'] = s.replace('\n', '')
            products.append(product)
        yield products
